data$timestamp <- as.POSIXct(data$timestamp, format="%H:%M:%S", tz="UTC")
coordinates <- data.frame(longitude = data$longitude, latitude = data$latitude)
# Define the range of possible cluster numbers
cluster_range <- 2:10
silhouette_scores <- numeric(length(cluster_range))
names(silhouette_scores) <- as.character(cluster_range)
# Loop through each possible number of clusters
set.seed(123)
for (k in cluster_range) {
# Perform k-means clustering
kmeans_result <- kmeans(coordinates, centers = k)
# Compute silhouette scores
distance_matrix <- dist(coordinates)
sil_scores <- silhouette(kmeans_result$cluster, distance_matrix)
# The average silhouette score for each k
silhouette_scores[k] <- mean(sil_scores[, "sil_width"])
}
# Display all average silhouette scores
print(silhouette_scores)
# Convert results to a data frame for plotting
results_df <- data.frame(ClusterCount = names(silhouette_scores),
AverageSilhouette = silhouette_scores)
# Plot average silhouette scores against cluster count
plot <- ggplot(results_df, aes(x = ClusterCount, y = AverageSilhouette)) +
geom_line(group = 1, colour = "blue") +
geom_point(aes(color = ClusterCount), size = 3) +
labs(title = "Average Silhouette Scores by Cluster Count",
x = "Number of Clusters",
y = "Average Silhouette Score") +
theme_minimal()
# Print the plot
print(plot)
# Set the optimal number of clusters based on previous analysis
optimal_clusters <- 3
cat("Optimal number of clusters based on prior analysis:", optimal_clusters, "\n")
# Perform k-means clustering with the chosen optimal number of clusters
final_kmeans <- kmeans(coordinates, centers = optimal_clusters)
# Add cluster information to the data
data$cluster <- final_kmeans$cluster
# Plotting the clusters
cluster_plot <- ggplot(data, aes(x = longitude, y = latitude, color = factor(cluster))) +
geom_point(alpha = 0.8, size = 3) +
ggtitle(paste("Map of Clusters with K =", optimal_clusters)) +
labs(color = "Cluster") +
theme_minimal()
# Save and display the cluster plot
ggsave("optimal_clusters.png", plot = cluster_plot, width = 8, height = 6, dpi = 300)
display_img <- grid::rasterGrob(readPNG("optimal_clusters.png"), interpolate = TRUE)
gridExtra::grid.arrange(display_img)
# Compute and plot silhouette scores for the chosen optimal number of clusters
optimal_sil_scores <- silhouette(final_kmeans$cluster, distance_matrix)
sil_plot <- fviz_silhouette(optimal_sil_scores)
install.packages("factoextra")
#install.packages("factoextra")
library(dplyr)
library(ggplot2)
library(sf)
library(cluster)
library(gridExtra) # for better visualization of clusters
library(png) # for displaying images
# Load the data
data <- read.csv("~/Desktop/Sensor_Measurements/sampled_data.csv")
# Rename columns for consistency
colnames(data) <- c("timestamp", "pressure", "latitude", "longitude")
# Convert timestamp to POSIXct
data$timestamp <- as.POSIXct(data$timestamp, format="%H:%M:%S", tz="UTC")
coordinates <- data.frame(longitude = data$longitude, latitude = data$latitude)
# Define the range of possible cluster numbers
cluster_range <- 2:10
silhouette_scores <- numeric(length(cluster_range))
names(silhouette_scores) <- as.character(cluster_range)
# Loop through each possible number of clusters
set.seed(123)
for (k in cluster_range) {
# Perform k-means clustering
kmeans_result <- kmeans(coordinates, centers = k)
# Compute silhouette scores
distance_matrix <- dist(coordinates)
sil_scores <- silhouette(kmeans_result$cluster, distance_matrix)
# The average silhouette score for each k
silhouette_scores[k] <- mean(sil_scores[, "sil_width"])
}
# Display all average silhouette scores
print(silhouette_scores)
# Convert results to a data frame for plotting
results_df <- data.frame(ClusterCount = names(silhouette_scores),
AverageSilhouette = silhouette_scores)
# Plot average silhouette scores against cluster count
plot <- ggplot(results_df, aes(x = ClusterCount, y = AverageSilhouette)) +
geom_line(group = 1, colour = "blue") +
geom_point(aes(color = ClusterCount), size = 3) +
labs(title = "Average Silhouette Scores by Cluster Count",
x = "Number of Clusters",
y = "Average Silhouette Score") +
theme_minimal()
# Print the plot
print(plot)
# Set the optimal number of clusters based on previous analysis
optimal_clusters <- 3
cat("Optimal number of clusters based on prior analysis:", optimal_clusters, "\n")
# Perform k-means clustering with the chosen optimal number of clusters
final_kmeans <- kmeans(coordinates, centers = optimal_clusters)
# Add cluster information to the data
data$cluster <- final_kmeans$cluster
# Plotting the clusters
cluster_plot <- ggplot(data, aes(x = longitude, y = latitude, color = factor(cluster))) +
geom_point(alpha = 0.8, size = 3) +
ggtitle(paste("Map of Clusters with K =", optimal_clusters)) +
labs(color = "Cluster") +
theme_minimal()
# Save and display the cluster plot
ggsave("optimal_clusters.png", plot = cluster_plot, width = 8, height = 6, dpi = 300)
display_img <- grid::rasterGrob(readPNG("optimal_clusters.png"), interpolate = TRUE)
gridExtra::grid.arrange(display_img)
# Compute and plot silhouette scores for the chosen optimal number of clusters
optimal_sil_scores <- silhouette(final_kmeans$cluster, distance_matrix)
sil_plot <- fviz_silhouette(optimal_sil_scores)
#install.packages("factoextra")
library(dplyr)
library(ggplot2)
library(sf)
library(cluster)
library(gridExtra) # for better visualization of clusters
library(png) # for displaying images
library(factoextra) # for visualizing clustering results
# Load the data
data <- read.csv("~/Desktop/Sensor_Measurements/sampled_data.csv")
# Rename columns for consistency
colnames(data) <- c("timestamp", "pressure", "latitude", "longitude")
# Convert timestamp to POSIXct
data$timestamp <- as.POSIXct(data$timestamp, format="%H:%M:%S", tz="UTC")
coordinates <- data.frame(longitude = data$longitude, latitude = data$latitude)
# Define the range of possible cluster numbers
cluster_range <- 2:10
silhouette_scores <- numeric(length(cluster_range))
names(silhouette_scores) <- as.character(cluster_range)
# Loop through each possible number of clusters
set.seed(123)
for (k in cluster_range) {
# Perform k-means clustering
kmeans_result <- kmeans(coordinates, centers = k)
# Compute silhouette scores
distance_matrix <- dist(coordinates)
sil_scores <- silhouette(kmeans_result$cluster, distance_matrix)
# The average silhouette score for each k
silhouette_scores[k] <- mean(sil_scores[, "sil_width"])
}
# Display all average silhouette scores
print(silhouette_scores)
# Convert results to a data frame for plotting
results_df <- data.frame(ClusterCount = names(silhouette_scores),
AverageSilhouette = silhouette_scores)
# Plot average silhouette scores against cluster count
plot <- ggplot(results_df, aes(x = ClusterCount, y = AverageSilhouette)) +
geom_line(group = 1, colour = "blue") +
geom_point(aes(color = ClusterCount), size = 3) +
labs(title = "Average Silhouette Scores by Cluster Count",
x = "Number of Clusters",
y = "Average Silhouette Score") +
theme_minimal()
# Print the plot
print(plot)
# Set the optimal number of clusters based on previous analysis
optimal_clusters <- 3
cat("Optimal number of clusters based on prior analysis:", optimal_clusters, "\n")
# Perform k-means clustering with the chosen optimal number of clusters
final_kmeans <- kmeans(coordinates, centers = optimal_clusters)
# Add cluster information to the data
data$cluster <- final_kmeans$cluster
# Plotting the clusters
cluster_plot <- ggplot(data, aes(x = longitude, y = latitude, color = factor(cluster))) +
geom_point(alpha = 0.8, size = 3) +
ggtitle(paste("Map of Clusters with K =", optimal_clusters)) +
labs(color = "Cluster") +
theme_minimal()
# Save and display the cluster plot
ggsave("optimal_clusters.png", plot = cluster_plot, width = 8, height = 6, dpi = 300)
display_img <- grid::rasterGrob(readPNG("optimal_clusters.png"), interpolate = TRUE)
gridExtra::grid.arrange(display_img)
# Compute and plot silhouette scores for the chosen optimal number of clusters
optimal_sil_scores <- silhouette(final_kmeans$cluster, distance_matrix)
sil_plot <- fviz_silhouette(optimal_sil_scores)
print(sil_plot)
ggsave("silhouette_scores.png", plot = sil_plot, width = 8, height = 6, dpi = 300)
# Save the data with cluster assignments
write.csv(data, "clustered_data.csv", row.names = FALSE)
# 1. Sampling Script
The sampling script will convert the data to a spatial format and compute distances along the road and sample the data every 5 meters, reducing duplication.
# Geospatial Data Analysis Scripts
This document outlines the descriptions of the required three scripts used in the analysis of the given data: sampling, mapping, and clustering.
# Bonus Script: Calculate Duplicates at 10 Meter Distance
# Load the necessary packages
library(dplyr)
library(geosphere)
# Read the data
data <- read.csv("~/Desktop/Sensor_Measurements/Barometer_withlocation.csv")
# Clean and prepare data
data <- data %>%
mutate(timestamp = as.POSIXct(timestamp, format="%H:%M:%S", tz="UTC")) %>%
rename(pressure = YourPressureColumnName, latitude = YourLatitudeColumnName, longitude = YourLongitudeColumnName) %>%
select(timestamp, pressure, latitude, longitude)
# Bonus Script: Visualize Sampled Atmospheric Pressure Measurements
# Load the necessary packages
library(dplyr)
library(ggplot2)
library(geosphere)
# Read the data
data <- read.csv("~/Desktop/Sensor_Measurements/Barometer_withlocation.csv")
# Clean and prepare data
data <- data %>%
mutate(timestamp = as.POSIXct(timestamp, format="%Y-%m-%d %H:%M:%S", tz="UTC")) %>%
rename(pressure = YourPressureColumnName, latitude = YourLatitudeColumnName, longitude = YourLongitudeColumnName) %>%
select(timestamp, pressure, latitude, longitude)
# Bonus Script: Visualize Sampled Atmospheric Pressure Measurements
# Load the necessary packages
library(dplyr)
library(ggplot2)
library(geosphere)
# Read the data
data <- read.csv("~/Desktop/Sensor_Measurements/Barometer_withlocation.csv")
# Rename columns for consistency
colnames(data) <- c("timestamp", "pressure", "latitude", "longitude")
# Visualize the initial data
initial_plot <- ggplot(data, aes(x = longitude, y = latitude)) +
geom_point(aes(color = pressure), size = 2) +
scale_color_gradient(low = "blue", high = "red") +
ggtitle("Scatter Plot of Atmospheric Pressure Measurements") +
xlab("Longitude") +
ylab("Latitude") +
theme_minimal()
# Bonus Script: Visualize Sampled Atmospheric Pressure Measurements
# Load the necessary packages
library(dplyr)
library(ggplot2)
library(geosphere)
# Read the data
data <- read.csv("~/Desktop/Sensor_Measurements/Barometer_withlocation.csv")
# Rename columns for consistency
colnames(data) <- c("timestamp", "pressure", "latitude", "longitude")
# Convert timestamp to POSIXct
data$timestamp <- as.POSIXct(data$timestamp, format="%H:%M:%S", tz="UTC")
# Function to sample data at equal distances and count duplicates
sample_at_equal_distances <- function(data, distance = 10) {
# Arrange data by timestamp
data <- data %>% arrange(timestamp)
# Initialize sampled data with the first observation
sampled_data <- data[1, ]
duplicates_count <- 0
for (i in 2:nrow(data)) {
last_sampled_point <- sampled_data[nrow(sampled_data), ]
current_point <- data[i, ]
# Calculate the distance between the last sampled point and the current point
dist <- distHaversine(c(last_sampled_point$longitude, last_sampled_point$latitude),
c(current_point$longitude, current_point$latitude))
# If the distance is greater than or equal to the specified distance, add the current point to sampled data
if (dist >= distance) {
sampled_data <- rbind(sampled_data, current_point)
} else {
duplicates_count <- duplicates_count + 1
}
}
list(sampled_data = sampled_data, duplicates_count = duplicates_count)
}
# Sample data at 10 meters and get the number of duplicates
result <- sample_at_equal_distances(data, 10)
# Bonus Script: Visualize Sampled Atmospheric Pressure Measurements
# Load the necessary packages
library(dplyr)
library(ggplot2)
library(geosphere)
# Read the data
data <- read.csv("~/Desktop/Sensor_Measurements/Barometer_withlocation.csv")
# Clean column names: Remove unnamed or improperly named columns and correct names
colnames(data) <- make.names(colnames(data), unique = TRUE)
data <- data[, !colnames(data) %in% c("NA", "X")]  # Remove columns named 'NA' or starting with 'X' which often denote unnamed columns
# Rename columns for consistency
colnames(data) <- c("timestamp", "pressure", "latitude", "longitude")
# Convert timestamp to POSIXct
data$timestamp <- as.POSIXct(data$timestamp, format="%H:%M:%S", tz="UTC")
# Function to sample data at equal distances and count duplicates
sample_at_equal_distances <- function(data, distance = 10) {
# Arrange data by timestamp
data <- data %>% arrange(timestamp)
# Initialize sampled data with the first observation
sampled_data <- data[1, ]
duplicates_count <- 0
for (i in 2:nrow(data)) {
last_sampled_point <- sampled_data[nrow(sampled_data), ]
current_point <- data[i, ]
# Calculate the distance between the last sampled point and the current point
dist <- distHaversine(c(last_sampled_point$longitude, last_sampled_point$latitude),
c(current_point$longitude, current_point$latitude))
# If the distance is greater than or equal to the specified distance, add the current point to sampled data
if (dist >= distance) {
sampled_data <- rbind(sampled_data, current_point)
} else {
duplicates_count <- duplicates_count + 1
}
}
list(sampled_data = sampled_data, duplicates_count = duplicates_count)
}
# Sample data at 10 meters and get the number of duplicates
result <- sample_at_equal_distances(data, 10)
# Bonus Script: Visualize Sampled Atmospheric Pressure Measurements
# Load the necessary packages
library(dplyr)
library(ggplot2)
library(geosphere)
# Read the data
data <- read.csv("~/Desktop/Sensor_Measurements/Barometer_withlocation.csv")
# Clean column names: Remove unnamed or improperly named columns and correct names
colnames(data) <- make.names(colnames(data), unique = TRUE)
data <- data[, !colnames(data) %in% c("NA", "X")]  # Remove columns named 'NA' or starting with 'X' which often denote unnamed columns
# Rename columns for consistency
colnames(data) <- c("timestamp", "pressure", "latitude", "longitude")
# Convert timestamp to POSIXct, ensure your timestamp format matches the data
data$timestamp <- as.POSIXct(data$timestamp, format="%Y-%m-%d %H:%M:%S", tz="UTC")
# Function to sample data at equal distances and count duplicates
sample_at_equal_distances <- function(data, distance = 10) {
# Arrange data by timestamp
data <- data %>% arrange(timestamp)
# Initialize sampled data with the first observation
sampled_data <- data[1, ]
duplicates_count <- 0
for (i in 2:nrow(data)) {
last_sampled_point <- sampled_data[nrow(sampled_data), ]
current_point <- data[i, ]
# Check for NA values in coordinates
if (is.na(last_sampled_point$longitude) | is.na(last_sampled_point$latitude) |
is.na(current_point$longitude) | is.na(current_point$latitude)) {
next  # Skip this iteration if there are NA coordinates
}
# Calculate the distance between the last sampled point and the current point
dist <- distHaversine(c(last_sampled_point$longitude, last_sampled_point$latitude),
c(current_point$longitude, current_point$latitude))
# Debugging output (optional, can be commented out after confirming it works)
print(paste("Distance between points:", dist))
# If the distance is greater than or equal to the specified distance, add the current point to sampled data
if (!is.na(dist) && dist >= distance) {
sampled_data <- rbind(sampled_data, current_point)
} else {
duplicates_count <- duplicates_count + 1
}
}
list(sampled_data = sampled_data, duplicates_count = duplicates_count)
}
# Sample data at 10 meters and get the number of duplicates
result <- sample_at_equal_distances(data, 10)
sampled_data <- result$sampled_data
duplicates_count <- result$duplicates_count
# Print the number of duplicates found
print(paste("Number of duplicates found at 10 meter distance:", duplicates_count))
#install packages using the following line of code
#install.packages("packgae_name")
# Load the packages
library(dplyr)
library(geosphere)
# Read the data
data <- read.csv("~/Desktop/Sensor_Measurements/Barometer_withlocation.csv")
#if Sensor_Measurements folder is added in another drive than desktop
#make sure to change the file directory accordingly
# Clean column names: Remove unnamed or improperly named columns and correct names
colnames(data) <- make.names(colnames(data), unique = TRUE)
data <- data[, !colnames(data) %in% c("NA", "X")]  # Remove columns named 'NA'
#or starting with 'X' which often denote unnamed columns
# Rename columns for consistency
colnames(data) <- c("timestamp", "pressure", "latitude", "longitude")
# Convert timestamp to POSIXct
data$timestamp <- as.POSIXct(data$timestamp, format="%H:%M:%S", tz="UTC")
# Function to sample data at equal distances
sample_at_equal_distances <- function(data, distance = 10) {
# Arrange data by timestamp
data <- data %>% arrange(timestamp)
# Initialize sampled data with the first observation
sampled_data <- data[1, ]
for (i in 2:nrow(data)) {
last_sampled_point <- sampled_data[nrow(sampled_data), ]
current_point <- data[i, ]
# Check for NA values in coordinates
if (is.na(last_sampled_point$longitude) | is.na(last_sampled_point$latitude) |
is.na(current_point$longitude) | is.na(current_point$latitude)) {
next  # Skip this iteration if there are NA coordinates
}
# Calculate the distance between the last sampled point and the current point
dist <- distHaversine(c(last_sampled_point$longitude, last_sampled_point$latitude),
c(current_point$longitude, current_point$latitude))
# Debugging output (optional, can be commented out after confirming it works)
print(paste("Distance between points:", dist))
# If the distance is greater than or equal to the specified distance, add the current point to sampled data
if (dist >= distance) {
sampled_data <- rbind(sampled_data, current_point)
}
}
return(sampled_data)
}
# Sample data at 10 meters
sampled_data <- sample_at_equal_distances(data, 10)
# Save the sampled data
write.csv(sampled_data, "~/Desktop/Sensor_Measurements/sampled_data.csv", row.names = FALSE)
#install packages using the following line of code
#install.packages("packgae_name")
# Load the packages
library(dplyr)
library(leaflet)
library(gstat)
library(sp)
# Read the data
data <- read.csv("~/Desktop/Sensor_Measurements/sampled_data.csv")
# Rename columns for consistency
colnames(data) <- c("timestamp", "pressure", "latitude", "longitude")
# Create a spatial data frame
coordinates(data) <- ~longitude+latitude
# Create an interpolation grid
bbox <- bbox(data)
grd <- expand.grid(
longitude = seq(from = bbox[1, 1], to = bbox[1, 2], by = 0.001),
latitude = seq(from = bbox[2, 1], to = bbox[2, 2], by = 0.001)
)
coordinates(grd) <- ~longitude+latitude
gridded(grd) <- TRUE
# Perform IDW interpolation
idw_result <- idw(pressure ~ 1, data, grd)
# Convert to a data frame for leaflet
idw_df <- as.data.frame(idw_result)
colnames(idw_df) <- c("longitude", "latitude", "pressure")
# High contrast color palette
high_contrast_palette <- colorRampPalette(c("navy", "blue", "green", "yellow", "red", "darkred"))(100)
# Create the color function
colorFunc <- colorNumeric(palette = high_contrast_palette, domain = idw_df$pressure)
# Update the leaflet color mapping and add legend
map <- leaflet(data = idw_df) %>%
addTiles() %>%  # Add default OpenStreetMap tiles
addCircleMarkers(~longitude, ~latitude,
color = ~colorFunc(pressure),
radius = 5,
popup = ~paste("Pressure:", pressure, "hPa")) %>%
addLegend("bottomright", pal = colorFunc, values = ~pressure,
title = "Pressure (hPa)",
labFormat = labelFormat(suffix = " hPa"))
# Save the map as an HTML widget
htmlwidgets::saveWidget(map, "pressure_map.html", selfcontained = TRUE)
#install packages using the following line of code
#install.packages("packgae_name")
#Load packages
library(dplyr)
library(ggplot2)
library(sf)
library(cluster)
library(gridExtra) # for better visualization of clusters
library(png) # for displaying images
library(factoextra) # for visualizing clustering results
# Load the data
data <- read.csv("~/Desktop/Sensor_Measurements/sampled_data.csv")
# Rename columns for consistency
colnames(data) <- c("timestamp", "pressure", "latitude", "longitude")
# Convert timestamp to POSIXct
data$timestamp <- as.POSIXct(data$timestamp, format="%H:%M:%S", tz="UTC")
coordinates <- data.frame(longitude = data$longitude, latitude = data$latitude)
# Define the range of possible cluster numbers
cluster_range <- 2:10
silhouette_scores <- numeric(length(cluster_range))
names(silhouette_scores) <- as.character(cluster_range)
# Loop through each possible number of clusters
set.seed(123)
for (k in cluster_range) {
# Perform k-means clustering
kmeans_result <- kmeans(coordinates, centers = k)
# Compute silhouette scores
distance_matrix <- dist(coordinates)
sil_scores <- silhouette(kmeans_result$cluster, distance_matrix)
# The average silhouette score for each k
silhouette_scores[k] <- mean(sil_scores[, "sil_width"])
}
# Display all average silhouette scores
print(silhouette_scores)
# Convert results to a data frame for plotting
results_df <- data.frame(ClusterCount = names(silhouette_scores),
AverageSilhouette = silhouette_scores)
# Plot average silhouette scores against cluster count
plot <- ggplot(results_df, aes(x = ClusterCount, y = AverageSilhouette)) +
geom_line(group = 1, colour = "blue") +
geom_point(aes(color = ClusterCount), size = 3) +
labs(title = "Average Silhouette Scores by Cluster Count",
x = "Number of Clusters",
y = "Average Silhouette Score") +
theme_minimal()
# Print the plot
print(plot)
# Set the optimal number of clusters based on previous analysis
optimal_clusters <- 3
cat("Optimal number of clusters based on prior analysis:", optimal_clusters, "\n")
# Perform k-means clustering with the chosen optimal number of clusters
final_kmeans <- kmeans(coordinates, centers = optimal_clusters)
# Add cluster information to the data
data$cluster <- final_kmeans$cluster
# Plotting the clusters
cluster_plot <- ggplot(data, aes(x = longitude, y = latitude, color = factor(cluster))) +
geom_point(alpha = 0.8, size = 3) +
ggtitle(paste("Map of Clusters with K =", optimal_clusters)) +
labs(color = "Cluster") +
theme_minimal()
# Save and display the cluster plot
ggsave("optimal_clusters.png", plot = cluster_plot, width = 8, height = 6, dpi = 300)
display_img <- grid::rasterGrob(readPNG("optimal_clusters.png"), interpolate = TRUE)
gridExtra::grid.arrange(display_img)
# Compute and plot silhouette scores for the chosen optimal number of clusters
optimal_sil_scores <- silhouette(final_kmeans$cluster, distance_matrix)
sil_plot <- fviz_silhouette(optimal_sil_scores)
print(sil_plot)
ggsave("silhouette_scores.png", plot = sil_plot, width = 8, height = 6, dpi = 300)
# Save the data with cluster assignments
write.csv(data, "clustered_data.csv", row.names = FALSE)
